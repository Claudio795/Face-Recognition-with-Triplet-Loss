{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Un nuovo inizio.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRY5b7G2RTPD"
      },
      "source": [
        "# E proviamoci dai!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXPXqSb3VgoX",
        "outputId": "cea4a00f-5f23-45c9-b4d5-d9c7dcba5643"
      },
      "source": [
        "pip install facenet-pytorch"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.7/dist-packages (2.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (1.19.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (7.1.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (0.9.1+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (2.23.0)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet-pytorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision->facenet-pytorch) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3kyGI_gRSry"
      },
      "source": [
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision import datasets, models\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "import collections\n",
        "import PIL.Image\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import os\n",
        "import random\n",
        "from google.colab import drive\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import glob\n",
        "import shutil\n",
        "from facenet_pytorch import MTCNN\n",
        "# from pathlib import Path"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0w__2zcRNGF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0251b819-1ef5-4342-8e78-bee0a35f04c0"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKotQcSxRd2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d4d7d8-cb76-4c41-ab64-5a636e746486"
      },
      "source": [
        "drive.mount(\"/content/drive\")\n",
        "root_dir = \"/content/drive/My Drive/IA_Project\"\n",
        "data_path = os.path.join(root_dir, \"LFW_DIR\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t3_soFwCUXw"
      },
      "source": [
        "# iperparametri\n",
        "img_size = 224\n",
        "crop_size = 128\n",
        "batch_size = 130"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC4pKQdeCUOA"
      },
      "source": [
        "# image transformation\n",
        "\n",
        "norm_mean = (0.485, 0.456, 0.406)\n",
        "norm_std = (0.229, 0.224, 0.225)\n",
        "#norm_mean = (0.5, 0.5, 0.5)\n",
        "#norm_std = (0.5, 0.5, 0.5)\n",
        "\n",
        "test_transform = T.Compose([\n",
        "    T.Resize(img_size),\n",
        "    T.CenterCrop(crop_size),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(norm_mean, norm_std),\n",
        "])\n",
        "\n",
        "train_transform = T.Compose([\n",
        "    T.Resize(img_size),\n",
        "    T.RandomCrop(crop_size),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(norm_mean, norm_std),\n",
        "])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyRo987YVsPl"
      },
      "source": [
        "# Funzione per bilanciare il dataset\n",
        "def balance_dataset(data_path):\n",
        "  people_paths = glob.glob(os.path.join(data_path, \"*\")) #lista dei path delle directory\n",
        "  print(len(people_paths))\n",
        "\n",
        "  #ciclo le directory contenute nella root\n",
        "  for path in people_paths:\n",
        "    directory = glob.glob(os.path.join(path, \"*\")) #lista dei path degli elementi contenuti nella directory corrente\n",
        "\n",
        "    if len(directory) < 5:\n",
        "      shutil.rmtree(path)\n",
        "    images_list = []\n",
        "    for image in directory:\n",
        "      images_list.append(image)\n",
        "    for i in range(5, len(images_list)):\n",
        "      os.remove(images_list[i]) #rimuovo le immagini in eccesso\n",
        "\n",
        "  return "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReFBlyWakE9B",
        "outputId": "9886abb9-68c0-4f97-839b-b0c61c72999d"
      },
      "source": [
        "balance_dataset(data_path)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3YxwLB5UPVE"
      },
      "source": [
        "Serve ora dividere il dataset nei tre da utilizzare per il training, il testing e la validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VWM0iR0KdML"
      },
      "source": [
        "# per cominciare ricavo un dizionario del tipo {label: [path immagine]}\n",
        "def get_dict(data_path):\n",
        "  people_paths = glob.glob(os.path.join(data_path, \"*\"))\n",
        "  label = 0\n",
        "  my_dict = {}\n",
        "\n",
        "  for path in people_paths:\n",
        "    directory = glob.glob(os.path.join(path, \"*\"))\n",
        "    for item in directory:\n",
        "      my_dict[item] = label\n",
        "    label += 1\n",
        "\n",
        "  return my_dict"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqAoAbbPX0nj",
        "outputId": "c47ff53e-25b6-4cd7-c3df-308031890cd1"
      },
      "source": [
        "my_dict = get_dict(data_path)\n",
        "print(len(my_dict))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az-CagMVKdKB"
      },
      "source": [
        "list_item = list(my_dict.items())\n",
        "random.shuffle(list_item)\n",
        "list_item_rand = dict(list_item)\n",
        "\n",
        "# Dataset fraction\n",
        "test_frac = 0.2\n",
        "val_frac = 0.2\n",
        "\n",
        "# data len\n",
        "num_data = len(my_dict)\n",
        "num_test = int(num_data*test_frac)\n",
        "num_val = int(num_data*val_frac)\n",
        "\n",
        "# split dict in train, test, val\n",
        "num_data = num_data - num_test\n",
        "test_items = dict(list(list_item_rand.items())[num_data:]) \n",
        "list_item_rand = dict(list(list_item_rand.items())[:num_data])\n",
        "\n",
        "num_data = num_data - num_val\n",
        "val_items = dict(list(list_item_rand.items())[num_data:]) \n",
        "train_items = dict(list(list_item_rand.items())[:num_data])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPxw9Y5HKdHx",
        "outputId": "20d075c8-0ca5-420e-b2ab-64ea9e343a0a"
      },
      "source": [
        "print(f\" number of train samples: {len(train_items)}\\n\",\n",
        "      f\"number of val samples: {len(val_items)}\\n\",\n",
        "      f\"number if test samples: {len(test_items)}\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " number of train samples: 1269\n",
            " number of val samples: 423\n",
            " number if test samples: 423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyVZSDrZWztB"
      },
      "source": [
        "detector = MTCNN(select_largest=False)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "520SBCU-ZjAL"
      },
      "source": [
        "class LFWDataset(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, class_images_dict, detector, transform=None):\n",
        "    self.transform = transform\n",
        "    self.detector = detector\n",
        "    self.class_images_dict = class_images_dict\n",
        "    self.list_of_tuples = self.get_list_of_tuples(self.class_images_dict)\n",
        "\n",
        "  # serve dopo per il getitem\n",
        "  def get_list_of_tuples(self, class_images_dict):\n",
        "    list_of_tuples = [(k, v) for k, v in class_images_dict.items()] \n",
        "    return list_of_tuples\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.list_of_tuples)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img_path, label = self.list_of_tuples[index]\n",
        "    img = PIL.Image.open(img_path)\n",
        "    face = self.detector(img)\n",
        "\n",
        "    if self.transform:\n",
        "      face = self.transform(face)\n",
        "\n",
        "    return face, label"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM4apUPmZi63",
        "outputId": "d854065a-936e-4824-c32a-1a05ed87af68"
      },
      "source": [
        "# get datasets\n",
        "train_dataset = LFWDataset(train_items, detector=detector, transform=train_transform)\n",
        "test_dataset = LFWDataset(test_items, detector=detector, transform=test_transform)\n",
        "val_dataset = LFWDataset(val_items, detector=detector, transform=train_transform)\n",
        "\n",
        "num_train = len(train_dataset)\n",
        "num_test = len(test_dataset)\n",
        "num_val = len(val_dataset)\n",
        "\n",
        "print(f\"Num. training samples: {num_train}\")\n",
        "print(f\"Num. test samples: {num_test}\")\n",
        "print(f\"Num. val samples: {num_val}\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num. training samples: 1269\n",
            "Num. test samples: 423\n",
            "Num. val samples: 423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZGBAHXQZi0E"
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=2, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=2, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2, shuffle=False)\n",
        "\n",
        "# Define dictionary of loaders\n",
        "loaders = {\"train\": train_loader,\n",
        "           \"val\": val_loader,\n",
        "           \"test\": test_loader}"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC4KYN5wcppY"
      },
      "source": [
        "def _pairwise_distances(embeddings, squared=False):\n",
        "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
        "\n",
        "    Args:\n",
        "        embeddings: tensor of shape (batch_size, embed_dim)\n",
        "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
        "                 If false, output is the pairwise euclidean distance matrix.\n",
        "\n",
        "    Returns:\n",
        "        pairwise_distances: tensor of shape (batch_size, batch_size)\n",
        "    \"\"\"\n",
        "    dot_product = torch.matmul(embeddings, embeddings.t())\n",
        "\n",
        "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
        "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
        "    # shape (batch_size,)\n",
        "    square_norm = torch.diag(dot_product)\n",
        "\n",
        "    # Compute the pairwise distance matrix as we have:\n",
        "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
        "    # shape (batch_size, batch_size)\n",
        "    distances = square_norm.unsqueeze(0) - 2.0 * dot_product + square_norm.unsqueeze(1)\n",
        "\n",
        "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
        "    distances[distances < 0] = 0\n",
        "\n",
        "    if not squared:\n",
        "        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
        "        # we need to add a small epsilon where distances == 0.0\n",
        "        mask = distances.eq(0).float()\n",
        "        distances = distances + mask * 1e-16\n",
        "\n",
        "        distances = (1.0 -mask) * torch.sqrt(distances)\n",
        "\n",
        "    return distances"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99kfEAPDcplD"
      },
      "source": [
        "def _get_anchor_positive_triplet_mask(labels, device):\n",
        "    \"\"\"Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
        "    Args:\n",
        "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
        "    Returns:\n",
        "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
        "    \"\"\"\n",
        "    # Check that i and j are distinct\n",
        "    indices_equal = torch.eye(labels.size(0)).bool().to(device)\n",
        "    indices_not_equal = ~indices_equal\n",
        "\n",
        "    # Check if labels[i] == labels[j]\n",
        "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
        "    labels_equal = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
        "\n",
        "    return labels_equal & indices_not_equal"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7j6PvITcpgk"
      },
      "source": [
        "def _get_anchor_negative_triplet_mask(labels):\n",
        "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
        "    Args:\n",
        "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
        "    Returns:\n",
        "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
        "    \"\"\"\n",
        "    # Check if labels[i] != labels[k]\n",
        "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
        "\n",
        "    return ~(labels.unsqueeze(0) == labels.unsqueeze(1))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMxSx41fdudN"
      },
      "source": [
        "def batch_hard_triplet_loss(labels, embeddings, margin, squared=False, device='cpu'):\n",
        "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
        "\n",
        "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
        "\n",
        "    Args:\n",
        "        labels: labels of the batch, of size (batch_size,)\n",
        "        embeddings: tensor of shape (batch_size, embed_dim)\n",
        "        margin: margin for triplet loss\n",
        "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
        "                 If false, output is the pairwise euclidean distance matrix.\n",
        "\n",
        "    Returns:\n",
        "        triplet_loss: scalar tensor containing the triplet loss\n",
        "    \"\"\"\n",
        "    # Get the pairwise distance matrix\n",
        "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
        "\n",
        "    # For each anchor, get the hardest positive\n",
        "    # First, we need to get a mask for every valid positive (they should have same label)\n",
        "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels, device).float()\n",
        "\n",
        "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
        "    anchor_positive_dist = mask_anchor_positive * pairwise_dist\n",
        "\n",
        "    # shape (batch_size, 1)\n",
        "    hardest_positive_dist, _ = anchor_positive_dist.max(1, keepdim=True)\n",
        "\n",
        "    # For each anchor, get the hardest negative\n",
        "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
        "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels).float()\n",
        "\n",
        "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
        "    max_anchor_negative_dist, _ = pairwise_dist.max(1, keepdim=True)\n",
        "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
        "\n",
        "    # shape (batch_size,)\n",
        "    hardest_negative_dist, _ = anchor_negative_dist.min(1, keepdim=True)\n",
        "\n",
        "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
        "    tl = hardest_positive_dist - hardest_negative_dist + margin\n",
        "    tl[tl < 0] = 0\n",
        "    triplet_loss = tl.mean()\n",
        "\n",
        "    return triplet_loss"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub6RlF8FduaS"
      },
      "source": [
        "# download di resnet34\n",
        "model = models.resnet34(pretrained=False)\n",
        "model = model.to(device)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_SyZG9eduWc"
      },
      "source": [
        "def trainLoop(epochs, device, model, lr=0.001):\n",
        "\n",
        "  # model.eval()\n",
        "  model.to(device) # model Ã¨ definito sopra\n",
        "\n",
        "  #optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "  # initialize loss and accuracy values\n",
        "  history_loss = {\"train\": [], \"val\": [], \"test\": []}\n",
        "  history_accuracy = {\"train\": [], \"val\": [], \"test\": []}\n",
        "  best_val_accuracy = 0\n",
        "  test_accuracy_at_best_val = 0\n",
        "\n",
        "  try:\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      sum_loss = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "      loss_count = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "      sum_accuracy = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "      accuracy_count = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "\n",
        "      for split in [\"train\", \"val\", \"test\"]:\n",
        "        if split == 'train':\n",
        "          model.train()\n",
        "          torch.set_grad_enabled(True)\n",
        "        else:\n",
        "          model.eval()\n",
        "          torch.set_grad_enabled(False)\n",
        "        for (anchor_imgs, anchor_labels) in loaders[split]:\n",
        "          # Move to CUDA\n",
        "          anchor_imgs = anchor_imgs.to(device)\n",
        "          anchor_labels = anchor_labels.to(device)\n",
        "          \n",
        "          # Compute output\n",
        "          anchors_out = model(anchor_imgs)\n",
        "          # Compute Loss\n",
        "          loss = batch_hard_triplet_loss(anchor_labels, anchors_out, margin=0.2, device=device)\n",
        "          # Update loss\n",
        "          sum_loss[split] += loss.item()\n",
        "          loss_count[split] += 1\n",
        "          # Compute accuracy \n",
        "          pred_labels = anchors_out.argmax(1)\n",
        "          # print(pred_labels)\n",
        "          correct = pred_labels.eq(anchor_labels).sum().item()\n",
        "          # print(correct)\n",
        "          batch_accuracy = correct/anchor_imgs.size(0) # potremmo provare a mettere anchor_imgs.size(1)\n",
        "          sum_accuracy[split] += batch_accuracy\n",
        "          accuracy_count[split] += 1\n",
        "\n",
        "          if split == 'train':\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "      # Compute epoch loss\n",
        "      epoch_loss = {split: sum_loss[split]/loss_count[split] for split in [\"train\", \"val\", \"test\"]}\n",
        "      epoch_accuracy = {split: sum_accuracy[split]/accuracy_count[split] for split in [\"train\", \"val\", \"test\"]}\n",
        "      # Update history\n",
        "      for split in [\"train\", \"val\", \"test\"]:\n",
        "        history_loss[split].append(epoch_loss[split])\n",
        "        history_accuracy[split].append(epoch_accuracy[split])\n",
        "\n",
        "      # Print info\n",
        "      print(\"------------------------------------------------------------------\")\n",
        "      print(f\"Epoch {epoch+1}:\",\n",
        "            f\"TrL={epoch_loss['train']:.4f},\",\n",
        "            f\"TrA={epoch_accuracy['train']:.4f},\",\n",
        "            f\"VL={epoch_loss['val']:.4f},\",\n",
        "            f\"VA={epoch_accuracy['val']:.4f},\",\n",
        "            f\"TeL={epoch_loss['test']:.4f},\",\n",
        "            f\"TeA={epoch_accuracy['test']:.4f},\")\n",
        "      print(\"------------------------------------------------------------------\")\n",
        "\n",
        "      if epoch_accuracy['val'] > best_val_accuracy:\n",
        "        best_val_accuracy = epoch_accuracy['val']\n",
        "        test_accuracy_at_best_val = epoch_accuracy['test']\n",
        "\n",
        "  except KeyboardInterrupt:\n",
        "      print(\"Interrupted\")\n",
        "  finally:\n",
        "      print(f\"Final val accuracy: {best_val_accuracy:.4f}\")\n",
        "      print(f\"Final test accuracy: {test_accuracy_at_best_val:.4f}\")\n",
        "      # Plot loss\n",
        "      plt.title(\"Loss\")\n",
        "      for split in [\"train\", \"val\", \"test\"]:\n",
        "        plt.plot(history_loss[split], label=split)\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "      # Plot accuracy\n",
        "      plt.title(\"Accuracy\")\n",
        "      for split in [\"train\", \"val\", \"test\"]:\n",
        "        plt.plot(history_accuracy[split], label=split)\n",
        "      plt.legend()\n",
        "      plt.show()"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FqQYpkF_duR3",
        "outputId": "85fab025-cc7f-4230-f5b7-2192d499f55b"
      },
      "source": [
        "trainLoop(100, device, model, lr=0.0001)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final val accuracy: 0.0000\n",
            "Final test accuracy: 0.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-b599d4ced50a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-46-b83193ae5c4b>\u001b[0m in \u001b[0;36mtrainLoop\u001b[0;34m(epochs, device, model, lr)\u001b[0m\n\u001b[1;32m     28\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m           \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0manchor_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m           \u001b[0;31m# Move to CUDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m           \u001b[0manchor_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manchor_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-38-3ffb03592e6c>\", line 23, in __getitem__\n    face = self.transform(face)\n  File \"/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\", line 60, in __call__\n    img = t(img)\n  File \"/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\", line 97, in __call__\n    return F.to_tensor(pic)\n  File \"/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\", line 102, in to_tensor\n    raise TypeError('pic should be PIL Image or ndarray. Got {}'.format(type(pic)))\nTypeError: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>\n"
          ]
        }
      ]
    }
  ]
}